{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise 1\n",
    "\n",
    "You goal is to model with a neural a function of the form\n",
    "$$ z = f \\left( (x - a)^2 + (y - b)^2 \\right), $$ \n",
    "so you are searching for a mapping from $\\mathbb{R}^2 \\rightarrow \\mathbb{R}$. You don't know the constant $a,b$ and the function $f$, but you have a collection of data points $(x,y,z)$. \n",
    "\n",
    "1. Start by implementing a simple NN with linear layer. How hard is it to fit the data? \n",
    "\n",
    "2. Find a way to encode the data characteristics in the model. Is it still hard to fit the data? You can start by assuming that you know a and b.\n",
    "\n",
    "3. Study the effect of the sample. Decrease the amount of data in the training set. What do you observe?\n",
    "\n",
    "4. What about the case where you don't know a and b?\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Autoreload is to always reload the imported python files.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Matplotlib inline allows to make plot inline with the notebook with Matplotlib\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import all packages\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "from poutyne import Model, SKLearnMetrics\n",
    "from sklearn.metrics import r2_score, median_absolute_error\n",
    "from utils import metric_flatten, get_poutyne_callbacks, saferm\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# These line allows to select the first GPU of the machine or the CPU is not GPU is present\n",
    "cuda_device = 0\n",
    "device = torch.device(\"cuda:%d\" % cuda_device if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Dataset generation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Assume you don't know the data generating function.\n",
    "a = 0.2 # a, b are known\n",
    "b = 0.8\n",
    "def func(xx,yy):\n",
    "    y = 2*np.cos( 3* ((xx-a)**2 + (yy-b)**2) )\n",
    "    return y\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lim = 1.5\n",
    "x = np.arange(-lim, lim, 0.03)\n",
    "y = np.arange(-lim, lim, 0.03)\n",
    "xx, yy = np.meshgrid(x, y, sparse=False)\n",
    "z = func(xx,yy)\n",
    "h = plt.contourf(x, y, z)\n",
    "plt.colorbar()\n",
    "plt.axis('scaled');\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## dataset creation\n",
    "def sample_generator_regression(n):\n",
    "    # RRandom sampling of points\n",
    "    # x = np.random.laplace(size=[n, 2]).astype(np.float32)\n",
    "    x = np.random.rand(n, 2).astype(np.float32)*3 -1.5\n",
    "    # compute the output\n",
    "    # we expand the dimension to have the size [n x 1], which will be the output shape of the NN.\n",
    "    y = np.expand_dims(func(x[:,0], x[:,1]),axis=1)\n",
    "    # Convertion to Pytorch tensors\n",
    "    return torch.tensor(x), torch.tensor(y)\n",
    "\n",
    "n_train = 1024\n",
    "\n",
    "train_data = sample_generator_regression(n_train)\n",
    "valid_data = sample_generator_regression(64)\n",
    "test_data = sample_generator_regression(128)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# It is very practical to transform our data into torch dataset\n",
    "train_dataset = TensorDataset(*train_data)\n",
    "valid_dataset = TensorDataset(*valid_data)\n",
    "test_dataset = TensorDataset(*test_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Linear layer NN."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Definition of a NN with 4 Linear layers\n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self, input_size = 2, output_size = 1, n_hidden=64):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, n_hidden)\n",
    "        self.linear2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear3 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear4 = nn.Linear(n_hidden, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = F.leaky_relu(self.linear1(x))\n",
    "        x2 = F.leaky_relu(self.linear2(x1))\n",
    "        x3 = F.leaky_relu(self.linear3(x2))\n",
    "        x4 = self.linear4(x3)\n",
    "        return x4\n",
    "\n",
    "network = LinearNet(input_size = 2, output_size = 1, n_hidden=64)\n",
    "# The package `torchsummary` allows to display a summary of the model\n",
    "summary(network, (2,))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define a poutyne model with optimizer, loss and metrics\n",
    "model = Model(network, 'adam', 'mse',\n",
    "              batch_metrics=[\"l1\"],\n",
    "              epoch_metrics=[ SKLearnMetrics(metric_flatten(r2_score)), \n",
    "                              SKLearnMetrics(metric_flatten(median_absolute_error))\n",
    "                            ],\n",
    "              device=device)\n",
    "\n",
    "# optimization paramters\n",
    "optimization_kwargs = {}\n",
    "optimization_kwargs[\"batch_size\"] = 8\n",
    "optimization_kwargs[\"epochs\"] = 15\n",
    "\n",
    "# train the model\n",
    "# note that the function will load the model with best validation score at the end\n",
    "history = model.fit_dataset(train_dataset, valid_dataset=valid_dataset, **optimization_kwargs) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluate the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "e = [int(v['epoch']) for v in history]\n",
    "val_loss = [v['val_loss'] for v in history]\n",
    "train_loss = [v['loss'] for v in history]\n",
    "plt.plot(e, train_loss, \"-x\" ,label=\"Training\")\n",
    "plt.plot(e, val_loss,\"-x\", label=\"Validation\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"MSE\")\n",
    "plt.legend();"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can compute the score on the test set.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.evaluate_dataset(test_dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also perform prediction on a grid of points an look at the results."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lim = 3\n",
    "x = np.arange(-lim, lim, 0.05)\n",
    "y = np.arange(-lim, lim, 0.05)\n",
    "xx, yy = np.meshgrid(x, y, sparse=False)\n",
    "Z1 = func(xx,yy)\n",
    "\n",
    "X = torch.tensor(np.array([xx.reshape(-1), yy.reshape(-1)]).T.astype(np.float32))\n",
    "Z2 = model.predict(X).reshape(xx.shape)\n",
    "\n",
    "vmin = np.min(Z1)\n",
    "vmax = np.max(Z1)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1,2,1)\n",
    "# h = plt.contourf(x, y, Z1, vmin=vmin, vmax=vmax)\n",
    "plt.imshow(Z1, vmin=vmin, vmax=vmax, extent=[-lim,lim,-lim,lim])\n",
    "plt.colorbar()\n",
    "plt.axis('scaled')\n",
    "plt.title(\"Ground truth\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(Z2, vmin=vmin, vmax=vmax, extent=[-lim,lim,-lim,lim])\n",
    "plt.colorbar()\n",
    "plt.axis('scaled')\n",
    "plt.title(\"Neural network\");"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Architecture adapted to the dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Definition of a NN with 4 Linear layers\n",
    "class TransformedLinearNet(nn.Module):\n",
    "    def __init__(self, n_hidden=64):\n",
    "        super(TransformedLinearNet, self).__init__()\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.linear1 = nn.Linear(1, n_hidden)\n",
    "        self.linear2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear3 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear4 = nn.Linear(n_hidden, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r = (x[:,:1]-self.a)**2 + (x[:,1:]-self.b)**2\n",
    "        x1 = F.leaky_relu(self.linear1(r))\n",
    "        x2 = F.leaky_relu(self.linear2(x1))\n",
    "        x3 = F.leaky_relu(self.linear3(x2))\n",
    "        x4 = self.linear4(x3)\n",
    "        return x4\n",
    "\n",
    "network2 = TransformedLinearNet( n_hidden=256)\n",
    "summary(network2, (2,))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define a poutyne model with optimizer, loss and metrics\n",
    "model2 = Model(network2, 'adam', 'mse',\n",
    "              batch_metrics=[\"l1\"],\n",
    "              epoch_metrics=[ SKLearnMetrics(metric_flatten(r2_score)), \n",
    "                              SKLearnMetrics(metric_flatten(median_absolute_error))\n",
    "                            ],\n",
    "              device=device)\n",
    "\n",
    "# optimization paramters\n",
    "optimization_kwargs = {}\n",
    "optimization_kwargs[\"batch_size\"] = 8\n",
    "optimization_kwargs[\"epochs\"] = 15\n",
    "\n",
    "# train the model\n",
    "# note that the function will load the model with best validation score at the end\n",
    "history = model2.fit_dataset(train_dataset, valid_dataset=valid_dataset, **optimization_kwargs) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model2.evaluate_dataset(test_dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lim = 1.5\n",
    "x = np.arange(-lim, lim, 0.02)\n",
    "y = np.arange(-lim, lim, 0.02)\n",
    "xx, yy = np.meshgrid(x, y, sparse=False)\n",
    "Z1 = func(xx,yy)\n",
    "\n",
    "X = torch.tensor(np.array([xx.reshape(-1), yy.reshape(-1)]).T.astype(np.float32))\n",
    "Z2 = model2.predict(X).reshape(xx.shape)\n",
    "\n",
    "vmin = np.min(Z1)\n",
    "vmax = np.max(Z1)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1,2,1)\n",
    "# h = plt.contourf(x, y, Z1, vmin=vmin, vmax=vmax)\n",
    "plt.imshow(Z1, vmin=vmin, vmax=vmax, extent=[-lim,lim,-lim,lim])\n",
    "plt.colorbar()\n",
    "plt.axis('scaled')\n",
    "plt.title(\"Ground truth\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(Z2, vmin=vmin, vmax=vmax, extent=[-lim,lim,-lim,lim])\n",
    "plt.colorbar()\n",
    "plt.axis('scaled')\n",
    "plt.title(\"Neural network\");"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Learning a and b."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Definition of a NN with 4 Linear layers\n",
    "class TransformedLinearNet2(nn.Module):\n",
    "    def __init__(self, n_hidden=64):\n",
    "        super(TransformedLinearNet2, self).__init__()\n",
    "        \n",
    "        self.a = nn.parameter.Parameter(torch.tensor([0.5]), requires_grad=True)\n",
    "        self.b = nn.parameter.Parameter(torch.tensor([0.5]), requires_grad=True)\n",
    "\n",
    "        self.linear1 = nn.Linear(1, n_hidden)\n",
    "        self.linear2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear3 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear4 = nn.Linear(n_hidden, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r = (x[:,:1]-self.a)**2 + (x[:,1:]-self.b)**2\n",
    "        x1 = F.leaky_relu(self.linear1(r))\n",
    "        x2 = F.leaky_relu(self.linear2(x1))\n",
    "        x3 = F.leaky_relu(self.linear3(x2))\n",
    "        x4 = self.linear4(x3)\n",
    "        return x4\n",
    "\n",
    "network3 = TransformedLinearNet2( n_hidden=256)\n",
    "summary(network3, (2,))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define a poutyne model with optimizer, loss and metrics\n",
    "model3 = Model(network3, 'adam', 'mse',\n",
    "              batch_metrics=[\"l1\"],\n",
    "              epoch_metrics=[ SKLearnMetrics(metric_flatten(r2_score)), \n",
    "                              SKLearnMetrics(metric_flatten(median_absolute_error))\n",
    "                            ],\n",
    "              device=device)\n",
    "\n",
    "# optimization paramters\n",
    "optimization_kwargs = {}\n",
    "optimization_kwargs[\"batch_size\"] = 8\n",
    "optimization_kwargs[\"epochs\"] = 15\n",
    "\n",
    "# train the model\n",
    "# note that the function will load the model with best validation score at the end\n",
    "history = model3.fit_dataset(train_dataset, valid_dataset=valid_dataset, **optimization_kwargs) \n",
    "model3.evaluate_dataset(test_dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model3.network.a, model3.network.b"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "lim = 1.5\n",
    "x = np.arange(-lim, lim, 0.02)\n",
    "y = np.arange(-lim, lim, 0.02)\n",
    "xx, yy = np.meshgrid(x, y, sparse=False)\n",
    "Z1 = func(xx,yy)\n",
    "\n",
    "X = torch.tensor(np.array([xx.reshape(-1), yy.reshape(-1)]).T.astype(np.float32))\n",
    "Z2 = model3.predict(X).reshape(xx.shape)\n",
    "\n",
    "vmin = np.min(Z1)\n",
    "vmax = np.max(Z1)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1,2,1)\n",
    "# h = plt.contourf(x, y, Z1, vmin=vmin, vmax=vmax)\n",
    "plt.imshow(Z1, vmin=vmin, vmax=vmax, extent=[-lim,lim,-lim,lim])\n",
    "plt.colorbar()\n",
    "plt.axis('scaled')\n",
    "plt.title(\"Ground truth\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(Z2, vmin=vmin, vmax=vmax, extent=[-lim,lim,-lim,lim])\n",
    "plt.colorbar()\n",
    "plt.axis('scaled')\n",
    "plt.title(\"Neural network\");"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e77879e7ba482baebf215f15929643f03e50a667c52f4e8743882ead31c04b9"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('oeaw-ai-summer-school-2021-CMalzwFa': pipenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}