{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise 2\n",
    "\n",
    "You are given a collection of measurement of the form\n",
    "$$ z = f \\left( x, y\\right) + \\epsilon, $$\n",
    "where $\\epsilon$ is some random noise following the distribution \n",
    "$$\\epsilon \\sim \\text{Laplace} \\left( \\mathbf{0},0.6\\mathbf{I} \\right).$$\n",
    "Your goal is to recover the mapping $f(x,y)$ for all $x,y \\in [-3,3]\\times [-3,3]$. \n",
    "\n",
    "1. Start by implementing a simple NN with linear layer and try to recover $f(x,y)$ using the Mean Square Error (MSE) loss. The provided test set give a good approximation of the performance of your model. Note that, in practice, we often do not have any way to compute this approximation.\n",
    "\n",
    "2. The MSE loss is not optimal for this problem. Can you find why? What would be a better loss? Using the **same** network, optimize with respect of another (better) loss?\n",
    "\n",
    "3. Look at the loss function of the training an the validation set. What can you deduct?\n",
    "\n",
    "4. What happens if $\\epsilon$ is Gaussian? Turn off the parameter `use_laplacian` and rerun the experiments.\n",
    "\n",
    "What do you conclude from this exercise?\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Autoreload is to always reload the imported python files.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Matplotlib inline allows to make plot inline with the notebook with Matplotlib\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import all packages\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "from poutyne import Model, SKLearnMetrics\n",
    "from sklearn.metrics import r2_score, median_absolute_error\n",
    "from utils import metric_flatten, get_poutyne_callbacks, saferm\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# These line allows to select the first GPU of the machine or the CPU is not GPU is present\n",
    "cuda_device = 0\n",
    "device = torch.device(\"cuda:%d\" % cuda_device if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Dataset generation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Assume you don't know the data generating function.\n",
    "\n",
    "def ground_truth(xx,yy):\n",
    "    y = np.sin(yy*2) + np.cos(xx*3)+ - np.abs(xx-0.5) \n",
    "    return y\n",
    "\n",
    "def noisy1(y):\n",
    "    return y + 0.6*np.random.randn(*y.shape).astype(y.dtype)\n",
    "\n",
    "def noisy2(y):\n",
    "    return y + 0.6*np.random.laplace(size=y.shape).astype(y.dtype)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lim = 3\n",
    "x = np.arange(-lim, lim, 0.03)\n",
    "y = np.arange(-lim, lim, 0.03)\n",
    "xx, yy = np.meshgrid(x, y, sparse=False)\n",
    "Z1 = ground_truth(xx,yy)\n",
    "Z2 = noisy1(Z1)\n",
    "Z3 = noisy2(Z1)\n",
    "\n",
    "vmin = np.min(Z1)\n",
    "vmax = np.max(Z1)\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(Z1, vmin=vmin, vmax=vmax, extent=[-lim,lim,-lim,lim])\n",
    "plt.colorbar()\n",
    "plt.axis('scaled')\n",
    "plt.title(\"Ground truth\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(Z2, vmin=vmin, vmax=vmax, extent=[-lim,lim,-lim,lim])\n",
    "plt.colorbar()\n",
    "plt.axis('scaled')\n",
    "plt.title(\"Gaussian noise\")\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(Z3, vmin=vmin, vmax=vmax, extent=[-lim,lim,-lim,lim])\n",
    "plt.colorbar()\n",
    "plt.axis('scaled')\n",
    "plt.title(\"Laplacian noise\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## dataset creation\n",
    "def sample_generator(n, no_noise=False, laplacian=False):\n",
    "    # Random sampling of points\n",
    "    if no_noise:\n",
    "        xx = np.arange(-lim, lim, 2*lim/np.sqrt(n))\n",
    "        yy = np.arange(-lim, lim, 2*lim/np.sqrt(n))\n",
    "        x = np.array(np.meshgrid(xx, yy, sparse=False)).reshape(2,-1).T.astype(np.float32)\n",
    "    else:\n",
    "        np.random.seed(0)\n",
    "        x = np.random.rand(n, 2).astype(np.float32)*2*lim -lim\n",
    "    # compute the output\n",
    "    # we expand the dimension to have the size [n x 1], which will be the output shape of the NN.\n",
    "    y = np.expand_dims(ground_truth(x[:,0], x[:,1]),axis=1)\n",
    "    if not(no_noise):\n",
    "        if laplacian:\n",
    "            y = noisy2(y)\n",
    "        else:\n",
    "            y = noisy1(y)\n",
    "    # Convertion to Pytorch tensors\n",
    "    return torch.tensor(x), torch.tensor(y)\n",
    "\n",
    "n_train = 1024\n",
    "\n",
    "use_laplacian = True\n",
    "\n",
    "train_data = sample_generator(n_train, laplacian=use_laplacian)\n",
    "valid_data = sample_generator(64, laplacian=use_laplacian)\n",
    "test_data = sample_generator(256, no_noise=True, laplacian=use_laplacian)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.scatter(train_data[0][:,0], train_data[0][:,1], c=train_data[1][:,0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# It is very practical to transform our data into torch dataset\n",
    "train_dataset = TensorDataset(*train_data)\n",
    "valid_dataset = TensorDataset(*valid_data)\n",
    "test_dataset = TensorDataset(*test_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Linear layer NN."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Definition of a NN with 4 Linear layers\n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self, input_size = 2, output_size = 1, n_hidden=64):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, n_hidden)\n",
    "        self.linear2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear3 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear4 = nn.Linear(n_hidden, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = F.leaky_relu(self.linear1(x))\n",
    "        x2 = F.leaky_relu(self.linear2(x1))\n",
    "        x3 = F.leaky_relu(self.linear3(x2))\n",
    "        x4 = self.linear4(x3)\n",
    "        return x4\n",
    "\n",
    "network = LinearNet(input_size = 2, output_size = 1, n_hidden=64)\n",
    "# The package `torchsummary` allows to display a summary of the model\n",
    "summary(network, (2,))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = Model(network, 'adam', 'mse',\n",
    "              batch_metrics=[\"l1\"],\n",
    "              epoch_metrics=[ SKLearnMetrics(metric_flatten(r2_score)), \n",
    "                              SKLearnMetrics(metric_flatten(median_absolute_error))\n",
    "                            ],\n",
    "              device=device)\n",
    "# optimization paramters\n",
    "optimization_kwargs = {}\n",
    "optimization_kwargs[\"batch_size\"] = 8\n",
    "optimization_kwargs[\"epochs\"] = 50\n",
    "\n",
    "# train the model\n",
    "# note that the function will load the model with best validation score at the end\n",
    "history = model.fit_dataset(train_dataset, valid_dataset=valid_dataset, **optimization_kwargs) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluate the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "e = [int(v['epoch']) for v in history]\n",
    "val_loss = [v['val_loss'] for v in history]\n",
    "train_loss = [v['loss'] for v in history]\n",
    "plt.plot(e, train_loss, \"-x\" ,label=\"Training\")\n",
    "plt.plot(e, val_loss,\"-x\", label=\"Validation\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"MSE\")\n",
    "plt.legend();"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can compute the score on the test set.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_mse, (test_l1, _, _) = model.evaluate_dataset(test_dataset)\n",
    "test_l1, test_mse"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also perform prediction on a grid of points an look at the results."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x = np.arange(-lim, lim, 0.05)\n",
    "y = np.arange(-lim, lim, 0.05)\n",
    "xx, yy = np.meshgrid(x, y, sparse=False)\n",
    "Z1 = ground_truth(xx,yy)\n",
    "\n",
    "X = torch.tensor(np.array([xx.reshape(-1), yy.reshape(-1)]).T.astype(np.float32))\n",
    "Z2 = model.predict(X).reshape(xx.shape)\n",
    "\n",
    "vmin = np.min(Z1)\n",
    "vmax = np.max(Z1)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(Z1, vmin=vmin, vmax=vmax, extent=[-lim,lim,-lim,lim])\n",
    "plt.colorbar()\n",
    "plt.axis('scaled')\n",
    "plt.title(\"Ground truth\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(Z2, vmin=vmin, vmax=vmax, extent=[-lim,lim,-lim,lim])\n",
    "plt.colorbar()\n",
    "plt.axis('scaled')\n",
    "plt.title(\"Neural network\");"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "network_l1 = LinearNet(input_size = 2, output_size = 1, n_hidden=64)\n",
    "model_l1 = Model(network_l1, 'adam', 'l1',\n",
    "              batch_metrics=[\"mse\"],\n",
    "              epoch_metrics=[ SKLearnMetrics(metric_flatten(r2_score)), \n",
    "                              SKLearnMetrics(metric_flatten(median_absolute_error))\n",
    "                            ],\n",
    "              device=device)\n",
    "\n",
    "# optimization paramters\n",
    "optimization_kwargs = {}\n",
    "optimization_kwargs[\"batch_size\"] = 8\n",
    "optimization_kwargs[\"epochs\"] = 50\n",
    "\n",
    "# train the model\n",
    "# note that the function will load the model with best validation score at the end\n",
    "history = model_l1.fit_dataset(train_dataset, valid_dataset=valid_dataset, **optimization_kwargs) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "e = [int(v['epoch']) for v in history]\n",
    "val_loss = [v['val_loss'] for v in history]\n",
    "train_loss = [v['loss'] for v in history]\n",
    "plt.plot(e, train_loss, \"-x\" ,label=\"Training\")\n",
    "plt.plot(e, val_loss,\"-x\", label=\"Validation\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"L1\")\n",
    "plt.legend();"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_l1 ,(test_mse, _, _) = model_l1.evaluate_dataset(test_dataset)\n",
    "test_l1, test_mse"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x = np.arange(-lim, lim, 0.05)\n",
    "y = np.arange(-lim, lim, 0.05)\n",
    "xx, yy = np.meshgrid(x, y, sparse=False)\n",
    "Z1 = ground_truth(xx,yy)\n",
    "\n",
    "X = torch.tensor(np.array([xx.reshape(-1), yy.reshape(-1)]).T.astype(np.float32))\n",
    "Z2 = model_l1.predict(X).reshape(xx.shape)\n",
    "\n",
    "vmin = np.min(Z1)\n",
    "vmax = np.max(Z1)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(Z1, vmin=vmin, vmax=vmax, extent=[-lim,lim,-lim,lim])\n",
    "plt.colorbar()\n",
    "plt.axis('scaled')\n",
    "plt.title(\"Ground truth\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(Z2, vmin=vmin, vmax=vmax, extent=[-lim,lim,-lim,lim])\n",
    "plt.colorbar()\n",
    "plt.axis('scaled')\n",
    "plt.title(\"Neural network\");"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e77879e7ba482baebf215f15929643f03e50a667c52f4e8743882ead31c04b9"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('oeaw-ai-summer-school-2021-CMalzwFa': pipenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}